## Recitation6-Dynamo
``学号：515030910036   姓名：吕嘉伟``

### Question 1
> 为什么要引入虚拟节点？它有什么问题？

判断哈希算法好坏的四个标准：
+ 平衡性
+ 单调性
+ 分散性
+ 负载

一致性哈希算法：将数据和机器都通过哈希算法映射到一个0~2^32-1空间的环上，按顺时针迁移方法，将数据存储在环上与之最近的机器上。

但以上方法不保证平衡性，为了解决这一问题，引入了“虚拟节点”——是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。

”虚拟节点”的优点：如果一个节点不可用(由于故障或日常维护)，这个节点处理的负载将均匀地分散在剩余的可用节点。当一个节点再次可用，或一个新的节点添加到系统中，新的可用节点接受来自其他可用的每个节点的负载量大致相当。一个节点负责的虚拟节点的数目可以根据其处理能力来决定，顾及到物理基础设施的异质性。

#### 转自知乎：
实体节点对应的虚拟节点在环上的分布是被打散了的。比如实体节点B对应的是虚拟节点B1,B2，实体节点C对应的虚拟节点是C1，C2，实体节点A对应的虚拟节点是A1，A2，那A1，A2，B1，B2，C1，C2在环上是交错出现的，可能是B1，A1，C1，B2，C2，A2，所以你说的如果C的节点挂了，相当于的C1，C2虚拟节点都不在了。那C1，C2上的数据是可能落在B2和A2上的，这样C的数据就分散在B节点和A节点上了。


### Question 2
> Dynamo试图解决多少其他问题？ 请尽可能多地列出问题的概要和相应的解决方案。

#### 来自论文中的表格：
|问题|技术|优势|
|---|---|----|
|数据动态划分到系统中的节点上去|一致性哈希算法|增量可伸缩性|
|高可用性和耐用性|将数据复制到多台主机上|
|写的高可用性|矢量时钟与读取过程中的协调|版本大小与更新速率脱钩|
|暂时性的失败处理|草率仲裁并暗示提交|提高高可用性和耐用性|
|永久故障恢复|使用merkle树的反熵|在后台同步不同的副本|


#### 整理：
1. 数据分片
+ 传统的一致性hash：将机器节点随机对应在hash ring上，数据key所对应的hash值所在位置顺时针遇到的第一个节点负责自己所在的range。
  + 优点：新节点加入，或旧节点退出时，只影响紧相邻的下一个节点
  + 缺点：负载不均匀且不同的机器性能的不同没有考虑到
+ 增加virtrual nodes的一致性hash：这时每个机器节点对应hash ring上的多个虚拟节点，可以根据机器性能方便的调节所负责的虚拟节点数
  + 优点：充分考虑机器性能的不同且可以做到负载均衡
  + 缺点：分片转移时，实现上需要对整个range进行遍历; 加入或删除节点时Merkle tree需要重新计算
+ 数据空间等分Q份，T个机器节点时，每个机器分得S个分片，其中Q=T*S
  + 优点：分片固定大小，可对应单个文件，因此容易加入或删除节点，且容易备份。

2. 分片备份
为了系统高可用，Dynamo的每个分片都有N个副本，存储在hash ring上顺时针方向的N个节点上。这N个节点称为该数据的preference list。其中的每一个节点都可以对接受针对该数据的操作请求。

3. 由于preference list中的每个节点都可以对同一个数据进行处理，且为了高可用，用户写请求返回前并没有将数据同步到所有分片。当有大量并行访问或故障发生时，集群中的不同机器看到的同一个数据状态可能不同，这时就发生了冲突。Dynamo引入vector clock来在一定程度上缓解冲突的发生，并最终由应用端对冲突进行合并。

```
vector clock := list{ (node, counter), ...}
node := 机器id
counter := 该数据在node上的处理序序号
```

+ vector clock通过列出在数据在每个节点上的处理序列来发现不同vector clock之间的因果关系，其中每个(node, counter)可以看做是一个分量。
+ 当某个vector clock的所有分量都小于另一个时，该vector clock便是另一个的因，可以被覆盖。Dynamo节点通过这种因果关系尽可能的处理冲突。
+ 没有因果关系的所有vector clock需要全部返回客户端，在应用端处理。

![](http://i.imgur.com/K7ChFub.png?1)


4. 读写过程
+ 客户端通过负载均衡代理或者自己维护数据到机器的映射关系，将请求最终交给preference list中的一个Dynamo节点处理，该节点称为coodinator
+ Dynamo采用类似Quorum的方式保证数据正确，即W+R>N。
+ Put流程：
  + coodinator生成新的数据版本，及vector clock分量
  + 本地保存新数据
  + 向preference list中的所有节点发送写入请求
  + 收到W-1个确认后向用户返回成功
+ Get流程
  + coodinator向preference list中所有节点请求数据版本
  + 等到R-1个答复
  + coodinator通过vector clock处理有因果关系的数据版本
  + 将不相关的所有数据版本返回用户

5. 发生错误保证暂时可用
Dynamo中用hinted handoff的方式保证在出现暂时的节点或网络故障时，集群依然可以正常提供服务。
+ 流程：
  + 节点失败时，会将其负责分片发送给一致性哈希环上下一个本来没有该分片的节点
  + 收到该分片的节点会将分片放到单独的空间，并成为该分片的处理节点，同时不断的通过hint检测原节点
  + 发现原节点可用时，将数据传回原节点并删除本地分片
+ 优点：
  + 避免了短暂的机器或网络故障造成的不可用

6. 降低分片同步数据传输量
通过上面所述，可以看出当故障发生或者有新节点加入、离开集群时，都涉及分片的拷贝和传输。希望能够快速检查分片中内容是否相同，并通过仅发送不同的部分来减少数据传输量。Dynamo采用Merkle Tree来解决这个问题。

+ Merkle tree：
  + 每个叶子节点对应一个数据项，并记录其hash值
  + 每个非叶子节点记录其所有子节点的hash值
+ 使用：
  + Dynamo为自己维护的每一个分片维护一个Merkle Tree
  + 需要比较分片是否相同时，自根向下的比较两个Merkle Tree的对应节点，可以快速发现并定位差异所在

7. 成员信息及故障检测
+ 考虑到节点失败无法恢复的情况并不常见，Dynamo加入或离开集群都需要手动通过命令完成；
+ 当有用户请求时，coordinator会发现不可达的节点，并用其他节点代替之，之后开始周期性探测其恢复；
+ Dynamo集群中的每台机器都会维护当前集群的成员及节点不可达等信息，这些信息通过gossip协议广播到整个集群；
+ 客户端可以通过任意一个节点获得并维护这种成员信息，从而精确的找到自己要访问的数据所在。


### Question 3
>For each solution, could you please figure out which scenarios are suitable, and which are not? List as many scenarios as you can, and offer your own solution if you have. Bring your ideas to the class for discussion.

#### 来自网络

针对负载平衡策略：
Dynamo的负载平衡取决于如何给每台机器分配虚拟节点号。由于集群环境的异构性，每台物理机器包含多个虚拟节点。一般有如下两种分配节点号的方法：

1. 随机分配。每台物理节点加入时根据其配置情况随机分配S个Token(节点号)。这种方法的负载平衡效果还是不错的，因为自然界的数据大致是比较随机的，虽然可能出现某段范围的数据特别多的情况（如baidu, sina等域名下的网页特别多），但是只要切分足够细，即S足够大，负载还是比较均衡的。这个方法的问题是可控性较差，新节点加入/离开系统时，集群中的原有节点都需要扫描所有的数据从而找出属于新节点的数据，Merkle Tree也需要全部更新；另外，增量归档/备份变得几乎不可能。

2. 数据范围等分+随机分配。为了解决方法1的问题，首先将数据的Hash空间等分为Q = N * S份 (N=机器个数，S=每台机器的虚拟节点数），然后每台机器随机选择S个分割点作为Token。和方法1一样，这种方法的负载也比较均衡，且每台机器都可以对属于每个范围的数据维护一个逻辑上的Merkle Tree，新节点加入/离开时只需扫描部分数据进行同步，并更新这部分数据对应的逻辑Merkle Tree，增量归档也变得简单。该方法的一个问题是对机器规模需要做出比较合适的预估，随着业务量的增长，可能需要重新对数据进行划分。

不管采用哪种方法，Dynamo的负载平衡效果还是值得担心的。


Dynamo的优点

1. 设计简单，组合利用P2P的各种成熟技术，模块划分好，代码复用程度高。

2. 分布式逻辑与单机存储引擎逻辑基本隔离。很多公司有自己的单机存储引擎，可以借鉴Dynamo的思想加入分布式功能。

3. NWR策略可以根据应用自由调整，这个思想已经被Google借鉴到其下一代存储基础设施中。

4. 设计上天然没有单点，且基本没有对系统时钟一致性的依赖。而在Google的单Master设计中，Master是单点，需要引入复杂的分布式锁机制来解决，且Lease机制需要对机器间时钟同步做出假设。

Dynamo的缺陷

1. 负载平衡相比单Master设计较不可控；负载平衡策略一般需要预估机器规模，不能无缝地适应业务动态增长。

2. 系统的扩展性较差。由于增加机器需要给机器分配DHT算法所需的编号，操作复杂度较高，且每台机器存储了整个集群的机器信息及数据文件的Merkle Tree信息，机器最大规模只能到几千台。

3. 数据一致性问题。多个客户端的写操作有顺序问题，而在GFS中可以通过只允许Append操作得到一个比较好的一致性模型。

4. 数据存储不是有序，无法执行Mapreduce；Mapreduce是目前允许机器故障，具有强扩展性的最好的并行计算模型，且有开源的Hadoop可以直接使用，Dynamo由于数据存储依赖Hash无法直接执行Mapreduce任务。
